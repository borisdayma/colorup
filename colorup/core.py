# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_core.ipynb (unless otherwise specified).

__all__ = ['PILImageLAB', 'RGBToLAB', 'TensorLAB', 'decodes', 'TensorL', 'TensorAB', 'Tuple_L_AB', 'Split_L_AB',
           'AdjustType', 'encodes', 'decodes', 'decodes', 'encodes', 'decodes']

# Cell
from fastai2.vision.all import *
from fastai2.callback.wandb import _make_plt
import PIL.ImageCms

# Cell
class PILImageLAB(PILImage):
    "PILImage in the LAB space"
    pass

# Cell
class RGBToLAB(Transform):
    "Convert a PILImage from RGB to LAB space"
    order = ToTensor.order - 1  # perform before ToTensor
    rgb_profile = PIL.ImageCms.createProfile("sRGB")
    lab_profile  = PIL.ImageCms.createProfile("LAB")
    transform_rgb2lab = PIL.ImageCms.buildTransform(
        inputProfile=rgb_profile, outputProfile=lab_profile, inMode='RGB', outMode='LAB')
    transform_lab2rgb = PIL.ImageCms.buildTransform(
        inputProfile=lab_profile, outputProfile=rgb_profile, inMode='LAB', outMode='RGB')

    def encodes(self, x):
        return PILImageLAB(PIL.ImageCms.applyTransform(x, self.transform_rgb2lab))

    def decodes(self, x):
        return PILImage(PIL.ImageCms.applyTransform(x, self.transform_lab2rgb))

# Cell
class TensorLAB(TensorImage):
    "Tensor for images in the LAB space"
    display = noop

    def show(self, **kwargs):
        displayTensor = self.display()
        pilImageLAB = ToTensor().decode(displayTensor)
        return RGBToLAB().decode(pilImageLAB).show(**kwargs)

# Add conversion to/from tensors
PILImageLAB._tensor_cls = TensorLAB  # used by `encodes`

@ToTensor
def decodes(self, o:TensorLAB): return PILImageLAB(Image.fromarray(np.uint8(o.permute(1,2,0)), mode='LAB'))

# Cell
class TensorL(TensorLAB):
    "Tensor containing the L channel of an image"
    def display(self):
        blank_channel = torch.zeros_like(self[0])
        return TensorLAB(torch.stack((self[0], blank_channel, blank_channel), dim=0))

# Cell
class TensorAB(TensorLAB):
    "Tensor containing A & B channels of an image"
    def display(self):
        "juxtapose both channels"
        blank_channel = torch.zeros_like(self[0])
        l_channel = torch.full_like(self[0], 128)
        img_A = torch.stack((l_channel, self[0], blank_channel), dim=0)
        img_B = torch.stack((l_channel, blank_channel, self[1]), dim=0)
        return TensorLAB(torch.cat((img_A, img_B), dim=2))

# Cell
class Tuple_L_AB(Tuple):
    "Tuple with L channel and A & B channels"
    def display(self):
        img_L, img_AB = self
        return TensorLAB(torch.cat((img_L, img_AB), dim=0))

# Cell
class Split_L_AB(ItemTransform):
    "Split TensorLAB into TensorL (input) & TensorAB (output)"
    order = ToTensor.order + 1  # perform after ToTensor
    def encodes(self, x):
        return Tuple_L_AB((TensorL(x[0][None]), TensorAB(x[1:])))
    def decodes(self, x):
        return Tuple_L_AB(x).display()

# Cell
class AdjustType(Transform):
    "Cast A & B channels to correct type to have continuous values"
    order = Split_L_AB.order + 1
    def encodes(self, x:(TensorAB)):
        return x.char()
    def decodes(self, x:(TensorAB)):
        return x.byte()

# Cell
@IntToFloatTensor
def encodes(self, o:TensorLAB):
    return o.float().div_(255.)

@IntToFloatTensor
def decodes(self, o:TensorAB):
    return o.mul_(255.).clamp(-128., 127.).char()

@IntToFloatTensor
def decodes(self, o:TensorL):
    return o.clamp(0., 1.).mul_(255.).byte()

# Cell
@Normalize
def encodes(self, x:TensorAB): return x

@Normalize
def decodes(self, x:TensorAB): return x

# Cell
@typedispatch
def show_batch(x:TensorL, y:TensorAB, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):
    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, add_vert=1, figsize=figsize, double=True)
    ctxs[0::2] = [b.show(ctx=c, title='Input', **kwargs) for b,c,_ in zip(samples.itemgot(0),ctxs[0::2],range(max_n))]
    ctxs[1::2] = [Tuple_L_AB((l, ab)).display().show(
        ctx=c, title='Target', **kwargs) for l,ab,c,_ in zip(samples.itemgot(0), samples.itemgot(1), ctxs[1::2],range(max_n))]
    return ctxs

# Cell
@typedispatch
def show_results(x:TensorL, y:TensorAB, samples, outs, ctxs=None, max_n=10, figsize=None, **kwargs):
    if ctxs is None: ctxs = get_grid(3*min(len(samples), max_n), ncols=3, figsize=figsize)
    ctxs[0::3] = [b.show(ctx=c, title='Input', **kwargs) for b,c,_ in zip(samples.itemgot(0),ctxs[0::3],range(max_n))]
    ctxs[1::3] = [Tuple_L_AB((l, ab)).display().show(
        ctx=c, title='Prediction', **kwargs) for l,ab,c,_ in zip(samples.itemgot(0), outs.itemgot(0), ctxs[1::3],range(max_n))]
    ctxs[2::3] = [Tuple_L_AB((l, ab)).display().show(
        ctx=c, title='Target', **kwargs) for l,ab,c,_ in zip(samples.itemgot(0), samples.itemgot(1), ctxs[2::3],range(max_n))]
    return ctxs

# Cell
@typedispatch
def wandb_process(x:TensorL, y, samples, outs):
    res_input, res_prediction, res_truth = [],[],[]
    for s,o in zip(samples, outs):
        fig_input = _make_plt(s[0])
        res_input.append(wandb.Image(fig_input, caption="Input"))
        plt.close(fig_input)

        fig_prediction = _make_plt(Tuple_L_AB((s[0], o[0])).display())
        res_prediction.append(wandb.Image(fig_prediction, caption="Prediction"))
        plt.close(fig_prediction)

        fig_truth = _make_plt(Tuple_L_AB((s[0], s[1])).display())
        res_truth.append(wandb.Image(fig_truth, caption="Ground Truth"))
        plt.close(fig_truth)

    return {"Inputs": res_input, "Predictions": res_prediction, "Ground Truth": res_truth}